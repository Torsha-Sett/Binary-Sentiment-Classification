{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlXt49ABpe2R"
      },
      "source": [
        "import pandas as pd    # to load dataset\n",
        "import numpy as np     # for mathematic equation\n",
        "from nltk.corpus import stopwords   # to get collection of stopwords\n",
        "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
        "from tensorflow.keras.models import Sequential     # the model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
        "from tensorflow.keras.models import load_model   # load saved model\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8AIjOPztAHN",
        "outputId": "3d523499-11e1-4a1e-8493-ca875a37b97a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRXLGMUXqWMk"
      },
      "source": [
        "# **Loading and cleaning dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5xLo-oaqPmY"
      },
      "source": [
        "english_stops = set(stopwords.words('english'))\n",
        "def load_dataset():\n",
        "    df = pd.read_csv('IMDB Dataset.csv')\n",
        "    x_data = df['review']       # Reviews/Input\n",
        "    y_data = df['sentiment']    # Sentiment/Output\n",
        "\n",
        "    # PRE-PROCESS REVIEW\n",
        "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
        "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
        "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
        "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
        "    \n",
        "    # ENCODE SENTIMENT -> 0 & 1\n",
        "    y_data = y_data.replace('positive', 1)\n",
        "    y_data = y_data.replace('negative', 0)\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "x_data, y_data = load_dataset()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paLx_F0aq0vv"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.5)\n",
        "def get_max_length():\n",
        "    review_length = []\n",
        "    for review in x_train:\n",
        "        review_length.append(len(review))\n",
        "\n",
        "    return int(np.ceil(np.mean(review_length)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw9DsxV-u1Gm",
        "outputId": "c3d47b79-c8dd-4c5f-805d-98ca6587842a"
      },
      "source": [
        "x_train.shape,x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000, 129), (25000, 129))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5TV_JaZsE_2"
      },
      "source": [
        "# Tokenize and Pad/Truncate Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk0ppq5XsQHE"
      },
      "source": [
        "A Neural Network only accepts numeric data, so we need to encode the reviews. I use tensorflow.keras.preprocessing.text.Tokenizer to encode the reviews into integers, where each unique word is automatically indexed (using fit_on_texts method) based on x_train.\n",
        "x_train and x_test is converted into integers using texts_to_sequences method.\n",
        "\n",
        "Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using tensorflow.keras.preprocessing.sequence.pad_sequences.\n",
        "\n",
        "post- pad or truncate the words in the back of a sentence\n",
        "\n",
        "pre- pad or truncate the words in front of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_NbV23XsEPX"
      },
      "source": [
        "# ENCODE REVIEW\n",
        "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
        "token.fit_on_texts(x_train)\n",
        "x_train = token.texts_to_sequences(x_train)\n",
        "x_test = token.texts_to_sequences(x_test)\n",
        "\n",
        "max_length = get_max_length()\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
        "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "total_words = len(token.word_index) + 1   # add 1 because of 0 padding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfkxHUhwtgvq"
      },
      "source": [
        "# Build Architecture/Model\n",
        "Embedding Layer: in simple terms, it creates word vectors of each word in the word_index and group words that are related or have similar meaning by analyzing other words around them.\n",
        "\n",
        "LSTM Layer: to make a decision to keep or throw away data by considering the current input, previous output, and previous memory. There are some important components in LSTM.\n",
        "\n",
        "Forget Gate- decides information is to be kept or thrown away\n",
        "\n",
        "Input Gate- updates cell state by passing previous output and current input into sigmoid activation function\n",
        "\n",
        "Cell State- calculate new cell state, it is multiplied by forget vector (drop value if multiplied by a near 0), add it with the output from input gate to update the cell state value.\n",
        "\n",
        "Ouput Gate- decides the next hidden state and used for predictions\n",
        "\n",
        "Dense Layer: compute the input with the weight matrix and bias (optional), and using an activation function. I use Sigmoid activation function for this work because the output is only 0 or 1.\n",
        "\n",
        "The optimizer is Adam and the loss function is Binary Crossentropy because again the output is only 0 and 1, which is a binary number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXnzu4kLt0wv",
        "outputId": "ca261365-acea-468c-f387-31f1c54ff758"
      },
      "source": [
        "# ARCHITECTURE\n",
        "EMBED_DIM = 32\n",
        "LSTM_OUT = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
        "model.add(LSTM(LSTM_OUT))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 129, 32)           2417280   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 2,442,177\n",
            "Trainable params: 2,442,177\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfTcBayLt8Ym",
        "outputId": "ea1af68a-6edc-450e-e49d-a1267b2184df"
      },
      "source": [
        "checkpoint = ModelCheckpoint(\n",
        "    'models/LSTM.h5',\n",
        "    monitor='accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size = 128, epochs = 5, callbacks=[checkpoint])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "196/196 [==============================] - 40s 191ms/step - loss: 0.5228 - accuracy: 0.7029\n",
            "\n",
            "Epoch 00001: accuracy improved from -inf to 0.70292, saving model to models/LSTM.h5\n",
            "Epoch 2/5\n",
            "196/196 [==============================] - 37s 191ms/step - loss: 0.2367 - accuracy: 0.9165\n",
            "\n",
            "Epoch 00002: accuracy improved from 0.70292 to 0.91648, saving model to models/LSTM.h5\n",
            "Epoch 3/5\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.1279 - accuracy: 0.9618\n",
            "\n",
            "Epoch 00003: accuracy improved from 0.91648 to 0.96180, saving model to models/LSTM.h5\n",
            "Epoch 4/5\n",
            "196/196 [==============================] - 37s 189ms/step - loss: 0.0798 - accuracy: 0.9801\n",
            "\n",
            "Epoch 00004: accuracy improved from 0.96180 to 0.98008, saving model to models/LSTM.h5\n",
            "Epoch 5/5\n",
            "196/196 [==============================] - 37s 188ms/step - loss: 0.0532 - accuracy: 0.9885\n",
            "\n",
            "Epoch 00005: accuracy improved from 0.98008 to 0.98852, saving model to models/LSTM.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f01edad7c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r640vDm3xMvc",
        "outputId": "64f1a639-2e5c-4bc9-eec1-42d1b04c22bb"
      },
      "source": [
        "results = model.fit(\n",
        " x_train, y_train,\n",
        " epochs= 5,\n",
        " batch_size = 100,\n",
        " validation_data = (x_test, y_test)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "250/250 [==============================] - 47s 182ms/step - loss: 0.0605 - accuracy: 0.9864 - val_loss: 0.5259 - val_accuracy: 0.8605\n",
            "Epoch 2/5\n",
            "250/250 [==============================] - 45s 182ms/step - loss: 0.0566 - accuracy: 0.9861 - val_loss: 0.5612 - val_accuracy: 0.8563\n",
            "Epoch 3/5\n",
            "250/250 [==============================] - 46s 183ms/step - loss: 0.0436 - accuracy: 0.9898 - val_loss: 0.5907 - val_accuracy: 0.8482\n",
            "Epoch 4/5\n",
            "250/250 [==============================] - 45s 182ms/step - loss: 0.0330 - accuracy: 0.9927 - val_loss: 0.6074 - val_accuracy: 0.8566\n",
            "Epoch 5/5\n",
            "250/250 [==============================] - 45s 180ms/step - loss: 0.0286 - accuracy: 0.9944 - val_loss: 0.6934 - val_accuracy: 0.8475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwuaC7KEuLjd",
        "outputId": "0382bd99-2f46-4113-c816-fc3e1c95ddb1"
      },
      "source": [
        "y_pred = model.predict_classes(x_test, batch_size = 128)\n",
        "#y_pred=np.argmax(model.predict(x_test, batch_size = 128), axis=-1)\n",
        "\n",
        "true = 0\n",
        "for i, y in enumerate(y_test):\n",
        "    if y == y_pred[i]:\n",
        "        true += 1\n",
        "\n",
        "print('Correct Prediction: {}'.format(true))\n",
        "print('Wrong Prediction: {}'.format(len(y_pred) - true))\n",
        "print('Accuracy: {}'.format(true/len(y_pred)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Correct Prediction: 21444\n",
            "Wrong Prediction: 3556\n",
            "Accuracy: 85.776\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}