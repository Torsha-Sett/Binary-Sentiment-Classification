{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_LSTM2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLJdI91EEMlQ",
        "outputId": "7da267c1-0be0-4d59-a0e1-949c2abbc9ee"
      },
      "source": [
        "import pandas as pd    # to load dataset\n",
        "import numpy as np     # for mathematic equation\n",
        "from nltk.corpus import stopwords   # to get collection of stopwords\n",
        "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
        "from tensorflow.keras.models import Sequential     # the model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
        "from tensorflow.keras.models import load_model   # load saved model\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-Gxp1jAE_Bl",
        "outputId": "d62bcab3-57cf-436d-b4c4-7d6c61f238ea"
      },
      "source": [
        "def load_dataset():\n",
        "    df = pd.read_csv('IMDB Dataset.csv')\n",
        "    x_data = df['review']       # Reviews/Input\n",
        "    y_data = df['sentiment']    # Sentiment/Output\n",
        "\n",
        "    # ENCODE SENTIMENT -> 0 & 1\n",
        "    y_data = y_data.replace('positive', 1)\n",
        "    y_data = y_data.replace('negative', 0)\n",
        "\n",
        "    return x_data, y_data\n",
        "\n",
        "x_data, y_data = load_dataset()\n",
        "df=pd.concat([x_data,y_data],axis=1)\n",
        "df.shape,df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 2), Index(['review', 'sentiment'], dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyUK-jRhF3DX",
        "outputId": "abe48310-f17f-4d53-a909-f2846c2ae800"
      },
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\")) \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
        "    text = text.lower()\n",
        "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
        "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
        "    text = [word for word in text if not word in stop_words]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "df['Processed_Reviews'] = df.review.apply(lambda x: clean_text(x))\n",
        "df.shape,df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50000, 3),\n",
              " Index(['review', 'sentiment', 'Processed_Reviews'], dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb8NJrxAJT45",
        "outputId": "dff7e84d-38b4-4377-d45b-ab23da341e34"
      },
      "source": [
        "train = df.loc[25000:,]\n",
        "test = df.loc[:24999:,]\n",
        "train.shape,test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000, 3), (25000, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Me4W8hMEpOV",
        "outputId": "3eb6706d-b09b-4d51-e546-80959e8f6626"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Convolution1D\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers\n",
        "\n",
        "max_features = 6000\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(train['Processed_Reviews'])\n",
        "list_tokenized_train = tokenizer.texts_to_sequences(train['Processed_Reviews'])\n",
        "\n",
        "maxlen = 130\n",
        "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
        "y = train['sentiment']\n",
        "\n",
        "embed_size = 128\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embed_size))\n",
        "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
        "model.add(GlobalMaxPool1D())\n",
        "model.add(Dense(20, activation=\"relu\"))\n",
        "model.add(Dropout(0.05))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 100\n",
        "epochs = 3\n",
        "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "200/200 [==============================] - 59s 188ms/step - loss: 0.5800 - accuracy: 0.7028 - val_loss: 0.3337 - val_accuracy: 0.8606\n",
            "Epoch 2/3\n",
            "200/200 [==============================] - 37s 183ms/step - loss: 0.2563 - accuracy: 0.8986 - val_loss: 0.3157 - val_accuracy: 0.8686\n",
            "Epoch 3/3\n",
            "200/200 [==============================] - 37s 185ms/step - loss: 0.1746 - accuracy: 0.9392 - val_loss: 0.3485 - val_accuracy: 0.8670\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f797c5ef6d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfBbqpVoG5Pv",
        "outputId": "7c383be8-33b6-48a5-dbc0-e599b6f2afad"
      },
      "source": [
        "#test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
        "y_test = test[\"sentiment\"]\n",
        "list_sentences_test = test[\"review\"]\n",
        "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
        "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
        "prediction = model.predict(X_te)\n",
        "y_pred = (prediction > 0.5)\n",
        "from sklearn.metrics import f1_score, confusion_matrix,accuracy_score\n",
        "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
        "print('accuracy: {0}'.format(accuracy_score(y_pred, y_test)))\n",
        "print('Confusion matrix:')\n",
        "confusion_matrix(y_pred, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-score: 0.8103323175156001\n",
            "accuracy: 0.79088\n",
            "Confusion matrix:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8604,  1306],\n",
              "       [ 3922, 11168]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}